/**
 * Chat Service Implementation
 *
 * Provides synchronous and streaming chat completion capabilities.
 * Based on SPARC specification Section 5.1 (TypeScript).
 */

import type { OllamaConfig } from '../../config/types.js';
import type { SimulationLayer } from '../../simulation/layer.js';
import type { ChatRequest, ChatResponse, ChatChunk } from '../../types/chat.js';
import type { Role } from '../../types/message.js';
import { OllamaError } from '../../types/errors.js';
import { NdjsonParser } from '../../transport/ndjson-parser.js';

/**
 * Dependencies for ChatService
 */
export interface ChatServiceDeps {
  config: OllamaConfig;
  simulation: SimulationLayer;
}

/**
 * Chat service for Ollama
 *
 * Provides methods for:
 * - Synchronous chat completions
 * - Streaming chat completions
 * - Request validation
 * - Model resolution
 */
export class ChatService {
  private readonly config: OllamaConfig;
  private readonly simulation: SimulationLayer;

  constructor(deps: ChatServiceDeps) {
    this.config = deps.config;
    this.simulation = deps.simulation;
  }

  /**
   * Create a synchronous chat completion
   *
   * Executes a complete chat request and returns the full response
   * once generation is complete.
   *
   * @param request - Chat completion request
   * @returns Promise resolving to complete chat response
   * @throws {OllamaError} If validation fails or request errors
   */
  async create(request: ChatRequest): Promise<ChatResponse> {
    // 1. Validate request (messages not empty, valid roles)
    this.validateRequest(request);

    // 2. Resolve model (use defaultModel if not specified)
    const resolvedRequest = this.resolveModel(request);

    // 3. Build request body with stream: false
    const body = {
      model: resolvedRequest.model!,
      messages: resolvedRequest.messages,
      format: resolvedRequest.format,
      options: resolvedRequest.options,
      stream: false,
      keep_alive: resolvedRequest.keep_alive,
    };

    // 4. Execute through simulation layer
    const response = await this.simulation.execute(
      'chat',
      body,
      async (transport) => {
        return transport.post('/api/chat', body);
      }
    );

    // 5. Parse and return response
    if (!response.body || typeof response.body !== 'object') {
      throw OllamaError.internalError(
        'Invalid response body from Ollama server',
        response.status
      );
    }

    return response.body as ChatResponse;
  }

  /**
   * Create a streaming chat completion
   *
   * Executes a chat request and yields response chunks as they
   * are generated by the model.
   *
   * @param request - Chat completion request
   * @yields Chat response chunks
   * @throws {OllamaError} If validation fails or request errors
   */
  async *createStream(request: ChatRequest): AsyncGenerator<ChatChunk, void, unknown> {
    // 1. Validate request
    this.validateRequest(request);

    // 2. Resolve model
    const resolvedRequest = this.resolveModel(request);

    // 3. Build request body with stream: true
    const body = {
      model: resolvedRequest.model!,
      messages: resolvedRequest.messages,
      format: resolvedRequest.format,
      options: resolvedRequest.options,
      stream: true,
      keep_alive: resolvedRequest.keep_alive,
    };

    // 4. Execute streaming through simulation layer
    const stream = await this.simulation.executeStreaming(
      'chat',
      body,
      (transport) => {
        return transport.postStreaming('/api/chat', body);
      }
    );

    // 5. Parse NDJSON and yield ChatChunk objects
    const parser = new NdjsonParser<ChatChunk>();
    yield* parser.parse(stream);
  }

  /**
   * Validate chat request
   *
   * Ensures:
   * - Messages array is not empty
   * - All message roles are valid ('system', 'user', 'assistant')
   *
   * @param request - Request to validate
   * @throws {OllamaError} With VALIDATION_ERROR code on failure
   */
  private validateRequest(request: ChatRequest): void {
    // Check messages not empty
    if (!request.messages || request.messages.length === 0) {
      throw OllamaError.validationError(
        'Messages cannot be empty',
        'messages',
        '[]'
      );
    }

    // Check all roles are valid ('system', 'user', 'assistant')
    const validRoles: Role[] = ['system', 'user', 'assistant'];
    for (let i = 0; i < request.messages.length; i++) {
      const message = request.messages[i];
      if (!message) {
        throw OllamaError.validationError(
          `Message at index ${i} is null or undefined`,
          `messages[${i}]`,
          'null'
        );
      }
      if (!validRoles.includes(message.role)) {
        throw OllamaError.validationError(
          `Invalid message role: ${message.role}. Must be 'system', 'user', or 'assistant'`,
          `messages[${i}].role`,
          message.role
        );
      }
    }
  }

  /**
   * Resolve model for request
   *
   * Uses the model specified in the request, or falls back to
   * the default model from configuration.
   *
   * @param request - Request with optional model
   * @returns Request with resolved model
   * @throws {OllamaError} If no model is specified and no default is configured
   */
  private resolveModel(request: ChatRequest): ChatRequest {
    // If model is already specified and non-empty, use it
    if (request.model && request.model.trim()) {
      return request;
    }

    // Use default model from config
    if (this.config.defaultModel && this.config.defaultModel.trim()) {
      return {
        ...request,
        model: this.config.defaultModel,
      };
    }

    // No model available - throw validation error
    throw OllamaError.validationError(
      'Model is required. Specify a model in the request or set a default model in the configuration.',
      'model',
      ''
    );
  }
}
